{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25040e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import operator\n",
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f47cb537",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer:\n",
    "    def __init__(self):\n",
    "        self.tokens = []\n",
    "        self.docToken = {}\n",
    "        self.document_vector={}\n",
    "        self.termfreq = {}\n",
    "        self.idf = {}\n",
    "        self.tfidf={}\n",
    "        self.query_vector = {}\n",
    "        self.stopWord = []\n",
    "        self.cosine = {}\n",
    "        \n",
    "    def Preprocessing(self):\n",
    "        #  Loading Data from files\n",
    "        Doc={}\n",
    "        for x in range(50):\n",
    "            x=x+1\n",
    "            file=\"ShortStories/\"+str(x)+\".txt\"\n",
    "            Doc[x]=open(file,'r',encoding='utf-8').read()\n",
    "            \n",
    "        #  Storing all the data at a single place\n",
    "        allDoc=\"\"\n",
    "        for x in range(50):\n",
    "            x=x+1\n",
    "            allDoc=allDoc+\" \\n\"+Doc[x]\n",
    "        #  Importing Stopwords\n",
    "        self.stopWord=open(\"files/Stopword-List.txt\").read()\n",
    "        \n",
    "        #  Tokenizing the documents and stop words\n",
    "        self.tokens=nltk.word_tokenize(allDoc)\n",
    "        self.stopWord=nltk.word_tokenize(self.stopWord)\n",
    "        \n",
    "        #  Creating unique tokens\n",
    "        self.tokens=list(set(self.tokens))\n",
    "        \n",
    "        #  Remove special characters\n",
    "        removetable=str.maketrans(\"\", \"\", \"'!@#$%^&*()_=-\\|][:';:,<.>/?`~\")\n",
    "        self.tokens=[x.translate(removetable) for x in self.tokens]\n",
    "        \n",
    "        #  Decapitalized\n",
    "        self.tokens=[element.lower() for element in self.tokens]\n",
    "        \n",
    "        # Removing StopWords\n",
    "        self.tokens=[x for x in self.tokens if x.isalnum() and x not in self.stopWord]\n",
    "        self.tokens=[element.lower() for element in self.tokens]\n",
    "        \n",
    "        #  Sorting Tokens\n",
    "        self.tokens=sorted(self.tokens)\n",
    "        \n",
    "        #  Document wise Tokenization\n",
    "        for x in range(1,50):\n",
    "            self.docToken[x]=nltk.word_tokenize(Doc[x])\n",
    "            \n",
    "        #  Remove Special characters documnet wise\n",
    "        removetable=str.maketrans(\"\", \"\", \"'!@#$%^&*()_=-\\|][:';:,<.>/?`~\")\n",
    "        for x in range(1,50):\n",
    "            self.docToken[x]=[y.translate(removetable) for y in self.docToken[x]]\n",
    "\n",
    "        #  Documnet wise sorting\n",
    "        for x in range(1,50):\n",
    "            self.docToken[x]=sorted(self.docToken[x])\n",
    "\n",
    "        #  Decaptilized document wise\n",
    "        for x in range(1,50):\n",
    "            self.docToken[x]=[element.lower() for element in self.docToken[x]]\n",
    "\n",
    "        for x in range(1,50):\n",
    "            self.docToken[x]=[y for y in self.docToken[x] if y.isalnum() and y not in self.stopWord]\n",
    "            \n",
    "        #  Calling Term freq function\n",
    "        self.Find_TermFreq()\n",
    "        \n",
    "        #  Calling idf function\n",
    "        self.Find_idf()\n",
    "        \n",
    "        # Calling tfidf function\n",
    "        self.Find_tfidf()\n",
    "        \n",
    "        #  Calling Query_Processing\n",
    "        self.Query_processing()\n",
    "        \n",
    "        \n",
    "    def Find_TermFreq(self):\n",
    "        #  Creating dictionary for each token and assign value zero\n",
    "        for x in range(1,50):\n",
    "            self.document_vector[x]=dict.fromkeys(self.tokens,0)\n",
    "            \n",
    "        #  Add frequncy for each term\n",
    "        for x in range(1,50):\n",
    "            for word in self.docToken[x]:\n",
    "                self.document_vector[x][word]+=1 \n",
    "                \n",
    "        #  tf\n",
    "        for x in range(1,50):\n",
    "            self.termfreq[x]={}\n",
    "            for word,count in self.document_vector[x].items():\n",
    "                self.termfreq[x][word]=count\n",
    "        #  Writing term frequencies in the file\n",
    "        term_frequency_file=open('files/Term Frequency.txt','w')\n",
    "        for elem in self.termfreq:\n",
    "            term_frequency_file.write(str(elem) + ':\\n' + str(self.termfreq[elem]) + '\\n\\n')\n",
    "        term_frequency_file.close()\n",
    "        \n",
    "    def Find_idf(self):\n",
    "        #  unique Token document wise\n",
    "        for x in range(1,50):\n",
    "            self.docToken[x]=set(self.docToken[x])\n",
    "            self.docToken[x]=list(set(self.docToken[x]))\n",
    "\n",
    "        wordDcount=dict.fromkeys(self.tokens,0)\n",
    "        for word in self.tokens:\n",
    "            for x in range(1,50):\n",
    "                if word in self.docToken[x]:\n",
    "                    wordDcount[word]+=1\n",
    "\n",
    "        #  finding idf            \n",
    "        for word in self.tokens:\n",
    "            if wordDcount[word]>0:\n",
    "                count=wordDcount[word]\n",
    "                if count>50:\n",
    "                    count=50\n",
    "            self.idf[word]=math.log(50/count)\n",
    "            \n",
    "        #  Writing idf in the file\n",
    "        idf_file=open('files/idf.txt','w')\n",
    "        for elem in self.idf:\n",
    "            idf_file.write(str(elem) + ':' + str(self.idf[elem]) + '\\n\\n')\n",
    "        idf_file.close()\n",
    "        \n",
    "    def Find_tfidf(self):\n",
    "        for x in range(1,50):\n",
    "            self.tfidf[x]={}\n",
    "            for word in self.document_vector[x]:\n",
    "                self.tfidf[x][word]=self.termfreq[x][word]*self.idf[word]\n",
    "                \n",
    "        #  Writing tfidf in the file\n",
    "        tfidf_file=open('files/tf idf.txt','w')\n",
    "        for elem in self.tfidf:\n",
    "            tfidf_file.write(str(elem) + ':' + str(self.tfidf[elem]) + '\\n\\n')\n",
    "        tfidf_file.close()\n",
    "        \n",
    "    def Query_processing(self):\n",
    "        #  Take query from frontend\n",
    "        e1.insert(15,\"crowd busy\")\n",
    "        query = e1\n",
    "#         query=e1.get()\n",
    "        \n",
    "        #  Tokenize Query\n",
    "        qt=nltk.word_tokenize(query)\n",
    "\n",
    "        #  Remove Special characters\n",
    "        removetable=str.maketrans(\"\", \"\", \"'!@#$%^&*()_=-\\|][:';:,<.>/?`~\")\n",
    "        qt=[x.translate(removetable) for x in qt]\n",
    "\n",
    "        #  Decapitalized\n",
    "        qt=[element.lower() for element in qt]\n",
    "\n",
    "        #  Removind stopwords and making unique\n",
    "        qt=[y for y in qt if y.isalnum() and y not in self.stopWord]\n",
    "        qt=list(set(qt))\n",
    "\n",
    "        self.query_vector=dict.fromkeys(self.tokens,0)\n",
    "        for word in qt:\n",
    "            try:\n",
    "                self.query_vector[word]+=1\n",
    "            except KeyError:\n",
    "                None\n",
    "\n",
    "        #  Query idf\n",
    "        for words in self.query_vector:\n",
    "            try:\n",
    "                self.query_vector[words]=self.query_vector[words]*self.idf[word]\n",
    "            except KeyError:\n",
    "                None\n",
    "                \n",
    "        self.cosine_sim(self)\n",
    "        \n",
    "        \n",
    "        lst={}\n",
    "        for items in self.cosine:\n",
    "            if items[1]>=0.005:\n",
    "                lst[items[0]]=items[1]\n",
    "\n",
    "        #-----------------------------\n",
    "        print(len(self.cosine)-len(lst))\n",
    "        print(lst)\n",
    "        lst1=list(lst.keys())\n",
    "\n",
    "        lst2=list(lst.values())\n",
    "        lst2=[\"%.5f\" % v for v in lst2]\n",
    "\n",
    "        e2.delete(0,END)\n",
    "        e3.delete(0,END)\n",
    "        e4.delete(0,END)\n",
    "\n",
    "        e2.insert(15,lst1)\n",
    "        e3.insert(15,lst2)\n",
    "        e4.insert(15,len(res)-len(lst))\n",
    "\n",
    "                \n",
    "    def cosine_sim(self):\n",
    "        temp=0\n",
    "        vec1=np.array([list(self.query_vector.values())])\n",
    "        for x in range(1,50):\n",
    "            vec2=np.array([list(self.tfidf[x].values())])\n",
    "            if cosine_similarity(vec1,vec2)>0:\n",
    "                temp=cosine_similarity(vec1,vec2)[0][0]\n",
    "                self.cosine[x]=temp\n",
    "\n",
    "        self.cosine=sorted(res.items(), key=operator.itemgetter(1), reverse=True)\n",
    "#         return self.cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "730e1c07",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'e1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-79973e3a2e5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mInd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIndexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mInd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-f6341a621e5a>\u001b[0m in \u001b[0;36mPreprocessing\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m#  Calling Query_Processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQuery_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-f6341a621e5a>\u001b[0m in \u001b[0;36mQuery_processing\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mQuery_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;31m#  Take query from frontend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0mquery\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;31m#  Tokenize Query\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'e1' is not defined"
     ]
    }
   ],
   "source": [
    "Ind = Indexer()\n",
    "Ind.Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ccace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
