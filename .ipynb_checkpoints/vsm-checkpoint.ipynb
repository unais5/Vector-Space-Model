{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c3e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "disturbed-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer:\n",
    "    vocab = []\n",
    "    def __init__(self):\n",
    "        self.inverted_index = {}\n",
    "        self.term_freq = {}\n",
    "        self.df = {}\n",
    "        self.idf = {}\n",
    "        self.tfidf_scr = {}\n",
    "        self.doc_dict = {}\n",
    "        self.stop_words = []\n",
    "        # self.tokenize_regex = r\"[\\!\\\"\\#\\$\\%\\&\\(\\)\\*\\+\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\]\\^\\_\\`\\{\\|\\}\\~\\\\ —",
    "]\"\n",
    "        self.tokenize_regex = r\"[^\\w',]\"\n",
    "        self.doc_count = 0\n",
    "\n",
    "    # read stopwords from file\n",
    "    def read_stop_words(self, path):\n",
    "        self.stop_words = []\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                for word in line.split():\n",
    "                    self.stop_words.append(word)\n",
    "\n",
    "    # read all documents in a path and tokenize and index the terms\n",
    "    def read_file(self, path):\n",
    "        word_list = []\n",
    "        self.read_stop_words('files/Stopword-List.txt')\n",
    "        # generate file list from directory provided\n",
    "        file_list = os.listdir(path)\n",
    "        self.doc_count = len(file_list)\n",
    "        file_list = sorted(file_list, key=lambda x: int(\n",
    "            \"\".join([i for i in x if i.isdigit()])))\n",
    "#         token_file = open('files/tokens.txt', 'w')  # file to store tokens\n",
    "        # open every document, and index the words in them\n",
    "        for doc_id, file_name in enumerate(file_list):\n",
    "            word_pos = 0\n",
    "            with open(path + file_name, 'r' , encoding=\"utf-8\") as file_data:\n",
    "                for line in file_data:\n",
    "                    # split and tokenize word by given characters\n",
    "                    for word in re.split(self.tokenize_regex, line):\n",
    "                        # remove trailing commas and apostrophes\n",
    "                        word = re.sub('[,\\'\\n]', '', word)\n",
    "                        word = word.lower()\n",
    "                        if word:\n",
    "#                             token_file.write(word + ' ')\n",
    "                            # add to invered index excluding stop words\n",
    "                            if word not in self.stop_words:\n",
    "                                word_list.append(word)\n",
    "                                self.inverted(word, doc_id+1)\n",
    "        vocab = list(set(word_list))\n",
    "        for doc_id, file_name in enumerate(file_list):\n",
    "            word_pos = 0\n",
    "            with open(path + file_name, 'r' , encoding=\"utf-8\") as file_data:\n",
    "                self.CreateDoc_dict(file_data,doc_id)\n",
    "        self.termFrequencyInDoc(vocab)\n",
    "\n",
    "        \n",
    "        \n",
    "    def CreateDoc_dict(self,file_data,doc_id):\n",
    "        data = file_data.read()\n",
    "        self.doc_dict[\"DOC-\" + str(doc_id+1)] = data\n",
    "        \n",
    "    # write positional and inverted index to file\n",
    "    def index_to_file(self, path):\n",
    "        inverted_index_file = open(path+'Inverted_index.txt', 'w')\n",
    "        for elem in self.inverted_index:\n",
    "            inverted_index_file.write(elem + ':\\n' + str(self.inverted_index[elem]) + '\\n\\n')\n",
    "\n",
    "    # insert token to inverted index\n",
    "    def inverted(self, word, doc_id):\n",
    "        if word not in self.inverted_index:  # if word not present then add new entry to dict\n",
    "            self.inverted_index[word] = []\n",
    "            self.inverted_index[word].append(doc_id)\n",
    "        elif doc_id not in self.inverted_index[word]:\n",
    "            self.inverted_index[word].append(doc_id)\n",
    "    \n",
    "    def termFrequencyInDoc(self,vocab):\n",
    "        for doc_id in self.doc_dict.keys():\n",
    "            self.term_freq[doc_id] = {}\n",
    "        for word in vocab:\n",
    "            for doc_id,doc in self.doc_dict.items():\n",
    "                self.term_freq[doc_id][word] = doc.count(word)\n",
    "        print(self.term_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "powered-hands",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ind = Indexer()\n",
    "Ind.read_file('ShortStories/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-pendant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
